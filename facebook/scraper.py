import asyncio
import re
from datetime import date, datetime
from pathlib import Path

import imagehash
import requests
from PIL import Image
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright

import shared_state
from database.models import Session, Business, AdCreative


# --- –ö–û–ù–§–Ü–ì–£–†–ê–¶–Ü–Ø –°–ö–†–ê–ü–ï–†–ê ---
MAIN_CONTENT_SELECTOR = '#mount_0_0_lg'
# URL-—à–∞–±–ª–æ–Ω –¥–ª—è –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏ —Ä–µ–∫–ª–∞–º–∏ Facebook
ADS_URL_TEMPLATE = 'https://www.facebook.com/ads/library/?active_status=active&ad_type=all&country=ALL&is_targeted_country=false&media_type=image_and_meme&search_type=page&view_all_page_id={}'

# –ú–∞–ø–∞ –¥–ª—è –ø–µ—Ä–µ—Ç–≤–æ—Ä–µ–Ω–Ω—è —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö –º—ñ—Å—è—Ü—ñ–≤ –Ω–∞ –∞–Ω–≥–ª—ñ–π—Å—å–∫—ñ
MONTH_MAP = {
    '—Å—ñ—á': 'Jan', '–ª—é—Ç': 'Feb', '–±–µ—Ä': 'Mar',
    '–∫–≤—ñ—Ç': 'Apr', '–∫–≤—ñ': 'Apr',
    '—Ç—Ä–∞–≤': 'May',
    '—á–µ—Ä–≤': 'Jun', '—á–µ—Ä': 'Jun',
    '–ª–∏–ø': 'Jul', '—Å–µ—Ä–ø': 'Aug', '–≤–µ—Ä': 'Sep', "—Å–µ—Ä": 'Aug',
    '–∂–æ–≤—Ç': 'Oct',"–∂–æ–≤": 'Oct', '–ª–∏—Å': 'Nov', '–≥—Ä—É–¥': 'Dec', "–≥—Ä—É": "Dec"
}


IMAGES_DIR = Path('images')
IMAGES_DIR.mkdir(parents=True, exist_ok=True)


def parse_start_date(text: str) -> date | None:
    """–í–∏—Ç—è–≥—É—î —Ç–∞ –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î –¥–∞—Ç—É –∑–∞–ø—É—Å–∫—É —Ä–µ–∫–ª–∞–º–∏ –∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ä—è–¥–∫–∞."""
    match = re.search(r'–ü–æ—á–∞—Ç–æ–∫ –ø–æ–∫–∞–∑—É:\s*(\d{1,2})\s+([–∞-—è“ë—î—ñ—ó]+)\s+(\d{4})', text, re.IGNORECASE)
    if not match:
        return None

    day, month_ukr, year = match.groups()
    month_ukr_lower = month_ukr.lower()
    month_eng = None

    for key, value in MONTH_MAP.items():
        if month_ukr_lower.startswith(key):
            month_eng = value
            break

    if not month_eng:
        print(f"  -> –ü–æ–º–∏–ª–∫–∞: –ù–µ–≤—ñ–¥–æ–º–∏–π –º—ñ—Å—è—Ü—å '{month_ukr}'. –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–∫–∞ –≤ MONTH_MAP.")
        return None

    try:
        date_str = f'{day} {month_eng} {year}'
        return datetime.strptime(date_str, '%d %b %Y').date()
    except ValueError as e:
        print(f"  -> –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É –¥–∞—Ç–∏ '{date_str}': {e}")
        return None


def download_image(ad_id: str, img_url: str) -> str | None:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –æ–¥–Ω–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø–æ–≤–µ—Ä—Ç–∞—î –ª–æ–∫–∞–ª—å–Ω–∏–π —à–ª—è—Ö."""
    local_path = IMAGES_DIR / f'{ad_id}.jpg'
    try:
        r = requests.get(img_url, timeout=15)
        r.raise_for_status()
        with open(local_path, 'wb') as f:
            f.write(r.content)
        print(f"    - –ó–±–µ—Ä–µ–∂–µ–Ω–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {local_path}")
        return str(local_path)
    except requests.exceptions.RequestException as req_err:
        print(f"    - –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è {ad_id}: {req_err}")
        return None


async def fetch_ads_for_business(page, business: Business):
    """
    –í–∏—Ç—è–≥—É—î —Ä–µ–∫–ª–∞–º–Ω—ñ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –±—ñ–∑–Ω–µ—Å—É, —Ä–µ–∞–ª—ñ–∑—É—é—á–∏ –≤—Å—é —Ñ—ñ–Ω–∞–ª—å–Ω—É –ª–æ–≥—ñ–∫—É.
    """
    url = ADS_URL_TEMPLATE.format(business.fb_page_id)
    print(f"  -> –ü–µ—Ä–µ—Ö—ñ–¥ –¥–æ URL –¥–ª—è –±—ñ–∑–Ω–µ—Å—É '{business.name}' (ID: {business.fb_page_id})")

    try:
        await page.goto(url, wait_until='domcontentloaded', timeout=30000)
    except Exception as e:
        print(f"  -> –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ URL: {e}")
        return

    try:
        print("  -> –®—É–∫–∞—é —Å–ø–ª–∏–≤–∞—é—á–µ –≤—ñ–∫–Ω–æ cookie...")
        cookie_button = page.locator(
            '//button[contains(translate(., "ABCDEFGHIJKLMNOPQRSTUVWXYZ", "abcdefghijklmnopqrstuvwxyz"), "allow all")]'
            '| //button[contains(translate(., "ABCDEFGHIJKLMNOPQRSTUVWXYZ", "abcdefghijklmnopqrstuvwxyz"), "accept all")]'
            '| //button[contains(translate(., "ABCDEFGHIJKLMNOPQRSTUVWXYZ", "abcdefghijklmnopqrstuvwxyz"), "–¥–æ–∑–≤–æ–ª–∏—Ç–∏")]'
        ).first
        await cookie_button.wait_for(timeout=7000)
        await cookie_button.click()
        print("  -> –°–ø–ª–∏–≤–∞—é—á–µ –≤—ñ–∫–Ω–æ cookie —É—Å–ø—ñ—à–Ω–æ –∑–∞–∫—Ä–∏—Ç–æ.")
        await page.wait_for_timeout(2000)
    except Exception:
        print("  -> –°–ø–ª–∏–≤–∞—é—á–µ –≤—ñ–∫–Ω–æ cookie –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–¥–æ–≤–∂—É—é...")

    print("  -> –ü—Ä–æ–∫—Ä—É—á—É—é —Å—Ç–æ—Ä—ñ–Ω–∫—É –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –æ–≥–æ–ª–æ—à–µ–Ω—å...")
    for _ in range(5):
        await page.evaluate('window.scrollTo(0, document.body.scrollHeight)')
        await asyncio.sleep(2)

    try:
        print("  -> –û—á—ñ–∫—É—é 5 —Å–µ–∫—É–Ω–¥, —â–æ–± —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –≥–∞—Ä–∞–Ω—Ç–æ–≤–∞–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–ª–∞—Å—è...")
        await page.wait_for_timeout(5000)
        print("  -> –û—Ç—Ä–∏–º—É—é HTML-–≤–º—ñ—Å—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏...")
        html = await page.content()
        soup = BeautifulSoup(html, 'html.parser')
    except Exception as e:
        print(f"  -> –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —Ñ—ñ–∫—Å–æ–≤–∞–Ω–æ–≥–æ –æ—á—ñ–∫—É–≤–∞–Ω–Ω—è –∞–±–æ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è –∫–æ–Ω—Ç–µ–Ω—Ç—É: {e}")
        screenshot_path = f"debug_screenshot_{business.fb_page_id}.png"
        await page.screenshot(path=screenshot_path)
        print(f"  -> –ó–±–µ—Ä–µ–∂–µ–Ω–æ —Å–∫—Ä—ñ–Ω—à–æ—Ç –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É: {screenshot_path}.")
        return

    # 1. –ó–±–∏—Ä–∞—î–º–æ –∫–∞—Ä—Ç–∫–∏ –¥–ª—è –≤—Å—ñ—Ö —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö ID –æ–≥–æ–ª–æ—à–µ–Ω—å
    ad_cards_map = {}
    id_elements = soup.find_all(string=re.compile(r'–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏:'))

    for id_element in id_elements:
        ad_id_match = re.search(r'–Ü–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä –±—ñ–±–ª—ñ–æ—Ç–µ–∫–∏:\s*(\d+)', id_element.string)
        if not ad_id_match: continue
        ad_id = ad_id_match.group(1)

        if ad_id in ad_cards_map: continue

        ad_card = None
        for parent in id_element.find_parents('div'):
            if parent.find('hr'):
                ad_card = parent
                break
        if ad_card:
            ad_cards_map[ad_id] = ad_card

    print(f"  -> –ó–Ω–∞–π–¥–µ–Ω–æ {len(ad_cards_map)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å –¥–ª—è '{business.name}'.")
    if not ad_cards_map:
        return

    today = date.today()
    session = Session()
    try:
        scraped_ad_ids = set(ad_cards_map.keys())

        # 2. –û–±—Ä–æ–±–ª—è—î–º–æ –∫–æ–∂–Ω–µ —É–Ω—ñ–∫–∞–ª—å–Ω–µ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è
        for ad_id, ad_card in ad_cards_map.items():
            existing_ad = session.query(AdCreative).filter_by(fb_ad_id=ad_id).first()

            if existing_ad:
                # –û–ù–û–í–õ–ï–ù–ù–Ø –Ü–°–ù–£–Æ–ß–û–ì–û
                existing_ad.last_seen = today
                existing_ad.is_active = True
                existing_ad.duration_days = (today - existing_ad.start_date).days + 1
                print(f"    - –û–Ω–æ–≤–ª–µ–Ω–æ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è: ID {ad_id}, –¥–Ω—ñ–≤: {existing_ad.duration_days}")
            else:
                # –î–û–î–ê–í–ê–ù–ù–Ø –ù–û–í–û–ì–û
                start_date = parse_start_date(ad_card.get_text(separator=' '))
                if not start_date:
                    print(f"  -> –ü–æ–º–∏–ª–∫–∞: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –¥–∞—Ç—É –ø–æ—á–∞—Ç–∫—É –≤ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—ñ {ad_id}.")
                    continue

                # –õ–æ–≥—ñ–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥—Ä—É–≥–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è
                all_img_tags = ad_card.find_all('img')
                img_url, local_path, image_hash_str = None, None, None

                if len(all_img_tags) > 1:
                    img_tag = all_img_tags[1]
                    src = img_tag.get('src')
                    if src:
                        img_url = src
                        local_path = download_image(ad_id, src)
                        # –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –ø–µ—Ä—Ü–µ–ø—Ç–∏–≤–Ω–æ–≥–æ —Ö–µ—à—É
                        if local_path:
                            try:
                                hash_value = imagehash.phash(Image.open(local_path))
                                image_hash_str = str(hash_value)
                            except Exception as e:
                                print(f"    - –ü–æ–º–∏–ª–∫–∞ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É —Ö–µ—à—É –¥–ª—è {local_path}: {e}")
                else:
                    print(f"    - –ü–æ–ø–µ—Ä–µ–¥–∂–µ–Ω–Ω—è: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –¥—Ä—É–≥–æ–≥–æ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –¥–ª—è –Ω–æ–≤–æ–≥–æ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è {ad_id}.")

                # –í–∏—Ç—è–≥–Ω–µ–Ω–Ω—è –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è —É —ñ–Ω—à–∏—Ö —Ä–µ–∫–ª–∞–º–∞—Ö
                similar_ads_count = 0
                card_text = ad_card.get_text()
                match = re.search(r'–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å—Å—è –≤\s+(\d+)\s+–æ–≥–æ–ª–æ—à–µ–Ω–Ω—è—Ö', card_text)
                if match:
                    similar_ads_count = int(match.group(1))

                new_ad = AdCreative(
                    fb_ad_id=ad_id,
                    business_id=business.id,
                    image_url=img_url,
                    local_path=local_path,
                    image_hash=image_hash_str,
                    similar_ads_count=similar_ads_count,
                    start_date=start_date,
                    last_seen=today,
                    is_active=True,
                    duration_days=(today - start_date).days + 1
                )
                session.add(new_ad)
                print(f"    - –î–æ–¥–∞–Ω–æ –Ω–æ–≤–µ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è: ID {ad_id}, —Å—Ö–æ–∂–∏—Ö: {similar_ads_count}, –¥–Ω—ñ–≤: {new_ad.duration_days}.")

        # 3. –î–µ–∞–∫—Ç–∏–≤–∞—Ü—ñ—è —Å—Ç–∞—Ä–∏—Ö –æ–≥–æ–ª–æ—à–µ–Ω—å
        active_db_ads = session.query(AdCreative).filter_by(business_id=business.id, is_active=True).all()
        for ad in active_db_ads:
            if ad.fb_ad_id not in scraped_ad_ids:
                ad.is_active = False
                ad.end_date = today
                print(f"    - –î–µ–∞–∫—Ç–∏–≤–æ–≤–∞–Ω–æ –æ–≥–æ–ª–æ—à–µ–Ω–Ω—è: ID {ad.fb_ad_id}.")

        session.commit()
        print(f"  -> –ó–º—ñ–Ω–∏ –¥–ª—è –±—ñ–∑–Ω–µ—Å—É '{business.name}' –∑–±–µ—Ä–µ–∂–µ–Ω–æ.")
    except Exception as e:
        session.rollback()
        print(f"  -> –ö–†–ò–¢–ò–ß–ù–ê –ü–û–ú–ò–õ–ö–ê –æ–±—Ä–æ–±–∫–∏ '{business.name}': {e}. –ó–º—ñ–Ω–∏ –≤—ñ–¥–∫–æ—á–µ–Ω–æ.")
    finally:
        session.close()


async def scrape_all():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è, —è–∫–∞ –∑–∞–ø—É—Å–∫–∞—î —Å–∫—Ä–∞–ø—ñ–Ω–≥ –¥–ª—è –≤—Å—ñ—Ö –±—ñ–∑–Ω–µ—Å—ñ–≤ –∑ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö."""

    # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ –Ω–µ –π–¥–µ –≤–∂–µ —Å–∫—Ä–∞–ø—ñ–Ω–≥
    if shared_state.is_scraping:
        print("üü° –°–ø—Ä–æ–±–∞ –∑–∞–ø—É—Å–∫—É —Å–∫—Ä–∞–ø—ñ–Ω–≥—É, –∞–ª–µ –ø–æ–ø–µ—Ä–µ–¥–Ω—ñ–π –ø—Ä–æ—Ü–µ—Å —â–µ –∞–∫—Ç–∏–≤–Ω–∏–π. –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ.")
        return

    # –í—Å—Ç–∞–Ω–æ–≤–ª—é—î–º–æ "–∑–∞–º–æ–∫"
    shared_state.is_scraping = True
    print("--- –ü—Ä–æ—Ü–µ—Å —Å–∫—Ä–∞–ø—ñ–Ω–≥—É —Ä–æ–∑–ø–æ—á–∞—Ç–æ, –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∑–∞–º–æ–∫. ---")

    try:
        session = Session()
        businesses = session.query(Business).all()
        session.close()

        if not businesses:
            print("–£ –±–∞–∑—ñ –¥–∞–Ω–∏—Ö –Ω–µ–º–∞—î –±—ñ–∑–Ω–µ—Å—ñ–≤ –¥–ª—è —Å–∫—Ä–∞–ø—ñ–Ω–≥—É.")
            return

        async with async_playwright() as p:
            browser = None
            try:
                browser = await p.chromium.launch(headless=True)
                page = await browser.new_page()

                for business in businesses:
                    print(f"–ü–æ—á–∏–Ω–∞—é —Å–∫—Ä–∞–ø—ñ–Ω–≥ –¥–ª—è –±—ñ–∑–Ω–µ—Å—É: '{business.name}'")
                    await fetch_ads_for_business(page, business)
                    print(f"–ó–∞–∫—ñ–Ω—á–µ–Ω–æ —Å–∫—Ä–∞–ø—ñ–Ω–≥ –¥–ª—è –±—ñ–∑–Ω–µ—Å—É: '{business.name}'\n")

            except Exception as e:
                print(f"–ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å —Ä–æ–±–æ—Ç–∏ –±—Ä–∞—É–∑–µ—Ä–∞: {e}")
            finally:
                if browser:
                    await browser.close()
                    print("–ë—Ä–∞—É–∑–µ—Ä Playwright –∑–∞–∫—Ä–∏—Ç–æ.")
    finally:
        # –©–æ–± –∑–∞–º–æ–∫ —Ç–æ—á–Ω–æ –∑–Ω—è–≤—Å—è
        shared_state.is_scraping = False
        print("--- –ü—Ä–æ—Ü–µ—Å —Å–∫—Ä–∞–ø—ñ–Ω–≥—É –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –∑–∞–º–æ–∫ –∑–Ω—è—Ç–æ. ---")

# —Ç–µ—Å—Ç–æ–≤–∏–π —Å–∫—Ä–∞–ø—ñ–Ω–≥
if __name__ == '__main__':
    asyncio.run(scrape_all())